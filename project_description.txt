THE CENTERPIECE: vLLM “INFERENCE OWNER” PROJECT (MUST SHIP)
This is the repo you link in every outreach. It proves “week-one contributor.”
3.1 MVP (hours)
Serve a model with vLLM (OpenAI-compatible endpoint OR FastAPI wrapper)
Streaming tokens endpoint
/health endpoint
Config flags exposed (examples):
max_model_len
max_num_seqs
dtype/quant choice if applicable
3.2 OWNER FEATURES 
These are the differentiators:
Cancellation: client disconnect stops generation + frees slot/resources
Backpressure: cap in-flight requests; return 429 when saturated
Metrics: TTFT, tokens/sec, p50/p95/p99 latency, queue depth, active reqs
Loadgen: concurrency sweep + prompt length sweep
Logging: write raw results to JSON/CSV
3.3 REPRODUCIBILITY (READ THIS: THIS IS THE POINT)
Your README must let a stranger reproduce results exactly.
Include:
Environment setup (copy/paste)
either pip/uv commands OR Docker (preferred if you can)
Exact versions
vLLM version or commit
Python version
model name/version
Hardware / driver
paste nvidia-smi output
GPU model, driver, CUDA
Exact run commands
server: the full command with flags
loadgen: full command used
plotting: full command used
If someone can’t reproduce your graphs with 3–5 commands, it’s not done.
3.4 REQUIRED OUTPUTS (3 GRAPHS + 5 INTERPRETATION BULLETS)
Graphs:
Throughput (tokens/sec) vs concurrency
Latency (p50/p95/p99) vs concurrency
VRAM vs (max_model_len, concurrency) OR vs (max_num_seqs, max_model_len)
Interpretation bullets (must be in README):
why throughput saturates where it does
why p99 spikes and where (queueing, KV, scheduler limits, etc.)
what KV cache did to VRAM and concurrency
what batching helped/hurt
the single knob that mattered most (max_num_seqs / max_model_len / quant / etc.)
Repo standards:
technical README (not tutorial vibes)
clean folder: server/, loadgen/, plots/, results/
include sample results + raw logs



