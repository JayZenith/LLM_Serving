================================================================================
                    COMPLETE TESTING GUIDE - GPU INSTANCE DEPLOYMENT
================================================================================

This guide walks you through deploying and testing every requirement from
project_description.txt on a GPU instance.

================================================================================
PART 1: INSTANCE ACCESS
================================================================================

Your instance details (from instance.txt):
  - SSH: ssh -p 35959 root@79.112.58.103
  - The instance has an RTX 4090 (24GB VRAM)

STEP 1: Connect to the instance
```bash
ssh -p 35959 root@79.112.58.103
```

If you get "connection refused", wait a few seconds and try again.
vast.ai instances sometimes need a moment after startup.

STEP 2: Verify GPU is available
```bash
nvidia-smi
```

Expected output:
  - GPU: NVIDIA GeForce RTX 4090
  - Memory: 24564MiB total
  - CUDA Version: 12.8

================================================================================
PART 2: PROJECT DEPLOYMENT
================================================================================

OPTION A: Copy from your local machine (recommended)
-------------------------------------------------
From your LOCAL machine (not the instance), run:

```bash
# Navigate to project directory
cd /home/jay-zenith/Desktop/LLM_Serving

# Copy everything to the instance
rsync -avz -e "ssh -p 35959" \
  --exclude '.git' \
  --exclude '__pycache__' \
  --exclude '.venv' \
  ./ root@79.112.58.103:/root/LLM_Serving/
```

OPTION B: Clone from GitHub
-------------------------------------------------
On the INSTANCE, run:

```bash
cd /root
git clone https://github.com/JayZenith/LLM_Serving.git
cd LLM_Serving
```

================================================================================
PART 3: INSTALL DEPENDENCIES
================================================================================

On the INSTANCE:

```bash
cd /root/LLM_Serving

# Install vLLM and dependencies
pip install vllm fastapi uvicorn aiohttp numpy matplotlib pandas

# Verify vLLM installed correctly
python3 -c "import vllm; print(f'vLLM {vllm.__version__} installed')"
```

Expected output: "vLLM 0.13.0 installed" (or newer)

================================================================================
PART 4: START THE SERVER
================================================================================

On the INSTANCE:

```bash
cd /root/LLM_Serving

# Start the server with Qwen2.5-1.5B
python3 server/server.py \
  --model Qwen/Qwen2.5-1.5B \
  --host 0.0.0.0 \
  --port 8000 \
  --max-model-len 2048 \
  --max-num-seqs 16 \
  --max-concurrent-requests 32
```

Wait for this output (takes ~30-60 seconds):
```
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

TIP: Run in background with:
```bash
nohup python3 server/server.py \
  --model Qwen/Qwen2.5-1.5B \
  --host 0.0.0.0 \
  --port 8000 \
  --max-model-len 2048 \
  --max-num-seqs 16 \
  --max-concurrent-requests 32 > /tmp/server.log 2>&1 &

# Check logs
tail -f /tmp/server.log
```

================================================================================
PART 5: TEST ALL REQUIREMENTS
================================================================================

Open a NEW terminal and SSH to the instance again:
```bash
ssh -p 35959 root@79.112.58.103
```

Now run these tests:

-----------------------------------------------------------------------------
TEST 1: Health Endpoint (/health)
-----------------------------------------------------------------------------
Requirement: "Health endpoint"

```bash
curl -s http://localhost:8000/health | python3 -m json.tool
```

EXPECTED OUTPUT:
```json
{
    "status": "healthy",
    "model": "Qwen/Qwen2.5-1.5B",
    "active_requests": 0,
    "queue_depth": 0
}
```

PASS CRITERIA:
  [x] Returns JSON with "status": "healthy"
  [x] Shows model name
  [x] Shows active_requests count
  [x] Shows queue_depth

-----------------------------------------------------------------------------
TEST 2: Generate Endpoint (non-streaming)
-----------------------------------------------------------------------------
Requirement: "Serve a model with vLLM"

```bash
curl -s -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is machine learning?", "max_tokens": 50}' | python3 -m json.tool
```

EXPECTED OUTPUT:
```json
{
    "text": " Machine learning is a subset of artificial intelligence...",
    "prompt_tokens": 5,
    "completion_tokens": 50,
    "ttft_ms": 25.5,
    "total_time_ms": 250.3
}
```

PASS CRITERIA:
  [x] Returns generated text
  [x] Shows prompt_tokens count
  [x] Shows completion_tokens count
  [x] Shows ttft_ms (time-to-first-token)
  [x] Shows total_time_ms

-----------------------------------------------------------------------------
TEST 3: Streaming Endpoint
-----------------------------------------------------------------------------
Requirement: "Streaming tokens endpoint"

```bash
curl -s -N -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Count from 1 to 5:", "max_tokens": 30, "stream": true}'
```

EXPECTED OUTPUT:
  - Tokens appear one at a time (not all at once)
  - You see the text build up progressively

PASS CRITERIA:
  [x] Tokens stream incrementally
  [x] Response completes successfully

-----------------------------------------------------------------------------
TEST 4: Metrics Endpoint
-----------------------------------------------------------------------------
Requirement: "Metrics: TTFT, tokens/sec, p50/p95/p99 latency, queue depth, active reqs"

```bash
curl -s http://localhost:8000/metrics | python3 -m json.tool
```

EXPECTED OUTPUT:
```json
{
    "request_count": 2,
    "active_requests": 0,
    "queue_depth": 0,
    "total_tokens_generated": 80,
    "error_count": 0,
    "ttft_ms": {
        "avg": 25.5,
        "p50": 25.0,
        "p95": 30.0,
        "p99": 35.0
    },
    "total_time_ms": {
        "avg": 250.0,
        "p50": 245.0,
        "p95": 280.0,
        "p99": 300.0
    },
    "tokens_per_second": 200.5
}
```

PASS CRITERIA:
  [x] Shows request_count
  [x] Shows active_requests
  [x] Shows queue_depth
  [x] Shows ttft_ms with avg, p50, p95, p99
  [x] Shows total_time_ms with avg, p50, p95, p99
  [x] Shows tokens_per_second

-----------------------------------------------------------------------------
TEST 5: Backpressure (429 Response)
-----------------------------------------------------------------------------
Requirement: "Backpressure: cap in-flight requests; return 429 when saturated"

```bash
cd /root/LLM_Serving

python3 -c "
import asyncio
import aiohttp

async def test_backpressure():
    async with aiohttp.ClientSession() as session:
        # Send 50 concurrent requests (server max is 32)
        tasks = []
        for i in range(50):
            tasks.append(session.post(
                'http://localhost:8000/generate',
                json={'prompt': 'Hello', 'max_tokens': 100}
            ))
        responses = await asyncio.gather(*tasks, return_exceptions=True)

        codes = {}
        for r in responses:
            if hasattr(r, 'status'):
                codes[r.status] = codes.get(r.status, 0) + 1
                await r.release()

        print(f'Response codes: {codes}')
        print(f'429 (rejected): {codes.get(429, 0)}')
        print(f'200 (accepted): {codes.get(200, 0)}')

        if codes.get(429, 0) > 0:
            print('BACKPRESSURE TEST: PASSED')
        else:
            print('BACKPRESSURE TEST: FAILED (no 429 responses)')

asyncio.run(test_backpressure())
"
```

EXPECTED OUTPUT:
```
Response codes: {200: 32, 429: 18}
429 (rejected): 18
200 (accepted): 32
BACKPRESSURE TEST: PASSED
```

PASS CRITERIA:
  [x] Some requests return 429 (rejected due to capacity)
  [x] Server doesn't crash
  [x] Accepted requests (200) complete successfully

-----------------------------------------------------------------------------
TEST 6: Cancellation
-----------------------------------------------------------------------------
Requirement: "Cancellation: client disconnect stops generation + frees slot"

```bash
# Start a long generation and cancel it with Ctrl+C after 1-2 seconds
timeout 2 curl -s -N -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Write a very long essay about history", "max_tokens": 500, "stream": true}'

echo ""
echo "Request was cancelled. Checking server health..."

# Verify server is still healthy and slot was freed
curl -s http://localhost:8000/health | python3 -m json.tool
```

PASS CRITERIA:
  [x] Server doesn't crash when client disconnects
  [x] active_requests returns to 0 after cancellation
  [x] Server remains healthy

-----------------------------------------------------------------------------
TEST 7: Config Flags
-----------------------------------------------------------------------------
Requirement: "Config flags exposed: max_model_len, max_num_seqs, dtype/quant"

The server accepts these flags (verify by checking help):
```bash
python3 server/server.py --help
```

EXPECTED FLAGS:
  --model             Model name/path
  --max-model-len     Maximum sequence length
  --max-num-seqs      Maximum concurrent sequences
  --dtype             Data type (auto, float16, bfloat16)
  --quantization      Quantization method (optional)
  --max-concurrent-requests  Backpressure threshold

PASS CRITERIA:
  [x] All config flags are documented in --help
  [x] Flags work when specified (server started with them)

================================================================================
PART 6: RUN FULL BENCHMARK
================================================================================

Requirement: "Loadgen: concurrency sweep + prompt length sweep"
Requirement: "Logging: write raw results to JSON/CSV"

```bash
cd /root/LLM_Serving

# Run concurrency sweep
python3 loadgen/loadgen.py \
  --url http://localhost:8000/generate \
  --prompt "Explain quantum computing in simple terms." \
  --max-tokens 100 \
  --concurrency-levels 1,2,4,8,16,32 \
  --requests 50 \
  --output results/benchmark_results.json
```

EXPECTED OUTPUT:
```
Starting benchmark against http://localhost:8000/generate
Mode: concurrency
...
Running benchmark: concurrency=1, num_requests=50
  TPS: 213.47, p99 TTFT: 13.06ms
Running benchmark: concurrency=2, num_requests=50
  TPS: 402.66, p99 TTFT: 10.59ms
...
Results saved to results/benchmark_results.json
```

PASS CRITERIA:
  [x] Benchmark completes without errors
  [x] Results saved to JSON file
  [x] Shows TPS and latency for each concurrency level
  [x] All requests succeed (no failures)

================================================================================
PART 7: GENERATE PLOTS
================================================================================

Requirement: "3 GRAPHS: Throughput vs concurrency, Latency vs concurrency, VRAM vs config"

```bash
cd /root/LLM_Serving

# Generate plots
python3 plots/plot_results.py \
  --results results/benchmark_results.json \
  --output-dir plots

# List generated files
ls -la plots/
```

EXPECTED FILES:
  - plots/throughput_vs_concurrency.png
  - plots/latency_vs_concurrency.png
  - plots/vram_usage.png

PASS CRITERIA:
  [x] All 3 PNG files generated
  [x] Files are non-empty (check file sizes > 10KB)

================================================================================
PART 8: COPY RESULTS BACK TO LOCAL
================================================================================

From your LOCAL machine:

```bash
cd /home/jay-zenith/Desktop/LLM_Serving

# Copy results and plots back
rsync -avz -e "ssh -p 35959" \
  root@79.112.58.103:/root/LLM_Serving/results/ ./results/

rsync -avz -e "ssh -p 35959" \
  root@79.112.58.103:/root/LLM_Serving/plots/*.png ./plots/
```

================================================================================
PART 9: VERIFICATION CHECKLIST
================================================================================

From project_description.txt, verify ALL of these:

MVP REQUIREMENTS:
  [ ] Serve a model with vLLM (OpenAI-compatible endpoint OR FastAPI wrapper)
  [ ] Streaming tokens endpoint
  [ ] /health endpoint
  [ ] Config flags: max_model_len, max_num_seqs, dtype/quant

OWNER FEATURES:
  [ ] Cancellation: client disconnect stops generation + frees slot
  [ ] Backpressure: cap in-flight requests; return 429 when saturated
  [ ] Metrics: TTFT, tokens/sec, p50/p95/p99 latency, queue depth, active reqs
  [ ] Loadgen: concurrency sweep + prompt length sweep
  [ ] Logging: write raw results to JSON/CSV

REQUIRED OUTPUTS:
  [ ] Graph 1: Throughput (tokens/sec) vs concurrency
  [ ] Graph 2: Latency (p50/p95/p99) vs concurrency
  [ ] Graph 3: VRAM vs (max_model_len, concurrency)
  [ ] 5 interpretation bullets in README

REPRODUCIBILITY:
  [ ] Environment setup (pip commands)
  [ ] Exact versions (vLLM, Python, model)
  [ ] Hardware/driver (nvidia-smi output in README)
  [ ] Exact run commands for server, loadgen, plotting
  [ ] Results reproducible in 3-5 commands

REPO STANDARDS:
  [ ] Technical README (not tutorial vibes)
  [ ] Clean folder: server/, loadgen/, plots/, results/
  [ ] Sample results + raw logs included

================================================================================
PART 10: QUICK REFERENCE COMMANDS
================================================================================

# Connect to instance
ssh -p 35959 root@79.112.58.103

# Check GPU
nvidia-smi

# Start server (foreground)
cd /root/LLM_Serving
python3 server/server.py --model Qwen/Qwen2.5-1.5B --host 0.0.0.0 --port 8000 --max-model-len 2048 --max-num-seqs 16 --max-concurrent-requests 32

# Start server (background)
nohup python3 server/server.py --model Qwen/Qwen2.5-1.5B --host 0.0.0.0 --port 8000 --max-model-len 2048 --max-num-seqs 16 --max-concurrent-requests 32 > /tmp/server.log 2>&1 &

# Check server logs
tail -f /tmp/server.log

# Test health
curl -s http://localhost:8000/health | python3 -m json.tool

# Test generate
curl -s -X POST http://localhost:8000/generate -H "Content-Type: application/json" -d '{"prompt": "Hello", "max_tokens": 20}' | python3 -m json.tool

# Test metrics
curl -s http://localhost:8000/metrics | python3 -m json.tool

# Run benchmark
python3 loadgen/loadgen.py --url http://localhost:8000/generate --concurrency-levels 1,2,4,8,16,32 --requests 50 --output results/benchmark_results.json

# Generate plots
python3 plots/plot_results.py --results results/benchmark_results.json --output-dir plots

# Stop server
pkill -f "server.py"

# Check what's using GPU
nvidia-smi
fuser -v /dev/nvidia*

# Kill GPU processes
pkill -9 python3

================================================================================
                              END OF TESTING GUIDE
================================================================================
